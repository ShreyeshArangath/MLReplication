{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum number of instances required for a perfectly balanced test data:  23\n",
      "The instances per digraph in testing data:  [23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23\n",
      " 23 23 23 23]\n",
      "The instances per digraph in training data:  [283 201  84 166 986 326 413 313 300 291  98 595 127  78 234 435 249 437\n",
      "  80 144 178 522 284 208 231 443 114 159 115 434 461 149 986 986 986 986\n",
      " 986 986 986 986 223 986 986 986 986 986 986  98 986 986 986 986 986 986\n",
      " 304 986 717 986 893 210 986 120 293 231 986 986  80 282 986 113 986 986\n",
      " 986 986 226 986 429 986 414 986 156 233 986 986 986 986 986 986 986 863\n",
      " 986 986 986 986 156 408 241 986 986 986 986 986 976 986 986 109 128 986\n",
      " 986 986 986 986 986 986 986 986 517 986 986  81 572 129 986 986 986 986\n",
      "  94 986 986 986 986 376 986 986 986 986 986 986 947 986 986 986 986 986\n",
      " 986 986 986 240 986 277 894 236 986 557 317 986 986 986 986 986 353  81\n",
      " 986 921 986 986 264 457 763 283 986 235 207 309 986 383 986 986 986 477\n",
      " 986 986 194 986 986 986 986 446 275 986 986 301 986 986 986 320 614 986\n",
      " 986 986 986 986 899 986 986 986 986 986 966 986  88 986 986 986 986 986\n",
      " 986 986 986 986 986 986 986 986 986 189 986 986 986 986 986 986 270 986\n",
      " 986 466 265  82 986 139 986  81 769 346 986 218 344 128 986 365 105 745\n",
      " 640 111 986 599 104 171 202 246 986 986 986 986 986 986 986 426 986 986\n",
      " 649 986 986 986 986 986 986 986 986 986 986 986 165 126 986 986 313 902\n",
      " 986 304 283 132 986 601 986 986 986 986 986 986 986 986 135 216  85 986\n",
      " 154 128 397 986 986 986 986 986 986 986 946 986 951 986 986 986 137 986\n",
      " 628  80 986 986 986 986 986 986 138 986  91 125 986 986 986 986 986 986\n",
      " 986 986 986 407 986 986 986 986 423 986 986 986 986 986 986 986 115 986\n",
      " 148 986 372 986 176 986 213 262 986 986 986  93 715 986 986 986 986 986\n",
      " 131 659 167 206 173 103 289 986 125 289 298 986 986 986 986 986 986 986\n",
      " 960 986 986 986 986 986 986 986 172 237 986 986 986 986 986 986 349 274\n",
      " 986 986 986 986 986 986 986 986 986 205 539 986 986 986 986 986 124 986\n",
      " 986 986 986 561 986 787 986 283 986 986 986 986 986 986 986 986 986 134\n",
      " 637 986 986 986 986 986 986 986 112 986 359 986  89 986 242 210 113 986\n",
      " 986 986 986 986 544 986 986 986  81 986 986 986 395 986  93 986 986 986\n",
      " 317 423 251 469 986 183 367 986  90 986 573 205 986 986 986 641 107 986\n",
      " 210 560 986 157 986 986 986 751 986 986 986 986 986 986 189 986  90 775\n",
      " 591 515 632 855 789 986 124 986 756 986 810 986 986 586 956 986  78 840\n",
      " 986 703 986 986 880 986 986 986 986 986 986 491 331  81 176 117]\n",
      "Accuracy Score : 1.5717860494213327%\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import RockYouDatasetParser \n",
    "import SystemPath\n",
    "from multiprocess import Pool\n",
    "import json \n",
    "import random\n",
    "\n",
    "path = SystemPath.Path()\n",
    "parser = RockYouDatasetParser.RockYouDatasetParser()\n",
    "\n",
    "def _getDigraphFrequencies(dataframe):\n",
    "    \"\"\"Returns the digraph frequencies within a dataframe.\"\"\"\n",
    "    return dataframe.groupby(\"digraph\").count()\n",
    "\n",
    "\"\"\" OBSOLETE \"\"\"\n",
    "def _perfectTestSplit(dataframe, testSize=0.2):\n",
    "    \"\"\"Returns a train dataframe and a perfectly split test dataframe.\"\"\"\n",
    "    uniqueDigraphSet = set(dataframe['digraph']) \n",
    "    testSplit = 0.2\n",
    "    minOccurencesForPerfectSplit = int(len(dataframe)*testSplit//len(uniqueDigraphSet))\n",
    "    trainDataframe = pd.DataFrame()\n",
    "    testDataframe = pd.DataFrame()\n",
    "    \n",
    "    for digraph in uniqueDigraphSet: \n",
    "        temp_dataframe = dataframe[dataframe['digraph']==digraph]\n",
    "        temp_dataframe = temp_dataframe.sample(frac=1).reset_index(drop=True)\n",
    "        temp_testDataframe = temp_dataframe.iloc[:minOccurencesForPerfectSplit,:]\n",
    "        temp_trainDataframe = temp_dataframe.iloc[minOccurencesForPerfectSplit:,:]\n",
    "        trainDataframe = pd.concat([trainDataframe, temp_trainDataframe], ignore_index=True, sort=False)\n",
    "        testDataframe = pd.concat([testDataframe, temp_testDataframe], ignore_index=True, sort=False)\n",
    "        \n",
    "    return trainDataframe, testDataframe\n",
    "\n",
    "def _getBinCount(numpyArray):\n",
    "    classes, indices = np.unique(numpyArray, return_inverse=True)\n",
    "    class_counts = np.bincount(indices)\n",
    "    return class_counts\n",
    "\n",
    "def _trainTestSplit(finalDataframe, testSize=0.2):\n",
    "    \"\"\"Returns xTrain, xTest, yTrain, yTest. The test data returned is perfectly balanced.\"\"\"\n",
    "    \n",
    "    uniqueDigraphSet = set(finalDataframe['digraph']) \n",
    "    xTrain, xTest , yTrain , yTest = train_test_split(X, y, test_size=testSize, random_state = 0)\n",
    "    min_instance_count = np.min(_getBinCount(yTest))\n",
    "    print(\"The minimum number of instances required for a perfectly balanced test data: \",min_instance_count)\n",
    "    \n",
    "    yTestBalanced = []\n",
    "    yTestDiscard = []\n",
    "    xTestBalanced = []\n",
    "    xTestDiscard = []\n",
    "    indices_to_discard = []   \n",
    "    for digraph in uniqueDigraphSet: \n",
    "        indices = (np.where(yTest==digraph))[0]\n",
    "        indices_to_keep = indices[:np.min(min_instance_count)]\n",
    "        indices_to_discard = indices[np.min(min_instance_count):]\n",
    "        yTestBalanced.extend(np.array(yTest)[indices_to_keep])\n",
    "        yTestDiscard.extend(np.array(yTest)[indices_to_discard])\n",
    "        xTestBalanced.extend(np.array(xTest)[indices_to_keep])\n",
    "        xTestDiscard.extend(np.array(xTest)[indices_to_discard])\n",
    "    \n",
    "    xTestDiscard= np.array(xTestDiscard)\n",
    "    xTestBalanced = np.array(xTestBalanced)\n",
    "    yTestBalanced = np.array(yTestBalanced)\n",
    "    yTestDiscard = np.array(yTestDiscard)\n",
    "    \n",
    "    xTrainUpdated = np.concatenate((xTrain, xTestDiscard))\n",
    "    yTrainUpdated = np.concatenate((yTrain, yTestDiscard))\n",
    "    \n",
    "    print(\"The instances per digraph in testing data: \", _getBinCount(yTestBalanced))\n",
    "    print(\"The instances per digraph in training data: \", _getBinCount(yTrainUpdated))\n",
    "    \n",
    "    return xTrainUpdated, xTestBalanced, yTrainUpdated, yTestBalanced\n",
    "\n",
    "    \n",
    "# Importing datasets\n",
    "msuDataset=pd.read_csv(path.getDataFilePath(\"msuupdated.csv\"))\n",
    "stonyDataset = pd.read_csv(path.getDataFilePath(\"stonybrooksdataset_updated.csv\"))\n",
    "greycWebDataset = pd.read_csv(path.getDataFilePath(\"greycwebdata.csv\"))\n",
    "greycDataset = pd.read_csv(path.getDataFilePath(\"greyc_normal.csv\"))\n",
    "rockYouDataframe = pd.read_csv(path.getDataFilePath(\"rockyou8subset.csv\"))\n",
    "\n",
    "# relevantDigraphDataframe = pd.read_csv(path.getDataFilePath(\"uniqueDigraphs.csv\"))\n",
    "originalRockYouDataframeWithCount = pd.read_csv(path.getDataFilePath(\"rockyoudataset.csv\"))\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "# Splitting the datasets for undersampling \n",
    "dataframe=pd.concat([ msuDataset ,greycWebDataset, greycDataset, stonyDataset])\n",
    "dataframe=dataframe.groupby(\"digraph\").filter(lambda x: len(x) > 100)\n",
    "dataframeWithLessThan1000Samples = dataframe.groupby(\"digraph\").filter(lambda x: len(x) < 1000) \n",
    "dataframeToUnderSample = dataframe.groupby(\"digraph\").filter(lambda x: len(x) >= 1000)\n",
    "\n",
    "# Under Sampling \n",
    "X=dataframeToUnderSample.iloc[:,2:].values\n",
    "y=dataframeToUnderSample.iloc[:,1].values\n",
    "undersample = NearMiss()\n",
    "xUnder, yUnder = undersample.fit_resample(X, y)\n",
    "# To delete the extra index column \n",
    "del dataframeWithLessThan1000Samples['Unnamed: 0'] \n",
    "undersampledDataframe = pd.DataFrame(xUnder, columns= ['inter-key','uut', 'ddt'])\n",
    "undersampledDataframe.insert(0, \"digraph\", yUnder)\n",
    "\n",
    "# Preprocessing \n",
    "finalDataframe = pd.concat([undersampledDataframe, dataframeWithLessThan1000Samples])\n",
    "X=finalDataframe.iloc[:,3:4].values\n",
    "y=finalDataframe.iloc[:,0].values\n",
    "_getDigraphFrequencies(finalDataframe)\n",
    "\n",
    "# Scaling features\n",
    "xTrain, xTest, yTrain, yTest = _trainTestSplit(finalDataframe, 0.30)\n",
    "xTestCopyForThreshold = xTest [:]\n",
    "\n",
    "classifier = RandomForestClassifier(random_state = 23)\n",
    "classifier.fit(xTrain, yTrain)\n",
    "\n",
    "yPred = classifier.predict(xTest)\n",
    "print (\"Accuracy Score : {}%\".format(accuracy_score(yTest, yPred)*100))\n",
    "\n",
    "### Preprocessing for experiments\n",
    "relevantDigraphDataframe = pd.DataFrame(finalDataframe['digraph'])\n",
    "relevantRockYouPasswords = parser.extractAllRelevantPasswords(rockYouDataframe, relevantDigraphDataframe)\n",
    "uniqueDigraphSet = set(finalDataframe['digraph']) \n",
    "\n",
    "\n",
    "def _valueAtIndices(value, array):\n",
    "    \"\"\" Returns a list of indices based on a given list of indices \"\"\"\n",
    "    return [index for index, val in enumerate(array) if val==value]\n",
    "\n",
    "def _getFeaturesAndLabelsForPassword(password, xTest, yTest):\n",
    "    \"\"\" Returns the test features and test labels for a given password \"\"\"\n",
    "    digraphArray = parser.getDigraphs(password)\n",
    "    testFeaturesForPassword = []\n",
    "    testLabels = []\n",
    "    for digraph in digraphArray: \n",
    "        occurencesOfDigraph = _valueAtIndices(digraph, yTest)\n",
    "        randomTestIndex = random.randint(occurencesOfDigraph[0], occurencesOfDigraph[-1])\n",
    "        testFeaturesForPassword.append(xTest[randomTestIndex])\n",
    "        testLabels.append(yTest[randomTestIndex])\n",
    "    testFeaturesForPassword = np.array(testFeaturesForPassword)\n",
    "    testLabels = np.array(testLabels)\n",
    "    \n",
    "    return testFeaturesForPassword, testLabels\n",
    "\n",
    "def _getFeaturesAndLabelsOffseted(password, xTest, yTest, offset = 0.2):\n",
    "    \"\"\" Returns the offseted test features and test labels for a given password \"\"\"\n",
    "    digraphArray = parser.getDigraphs(password)\n",
    "    testFeaturesForPassword = []\n",
    "    testLabels = []\n",
    "    for digraph in digraphArray: \n",
    "        occurencesOfDigraph = _valueAtIndices(digraph, yTest)\n",
    "        randomTestIndex = random.randint(occurencesOfDigraph[0], occurencesOfDigraph[-1])\n",
    "        testFeaturesForPassword.append(xTest[randomTestIndex])\n",
    "        testLabels.append(yTest[randomTestIndex])\n",
    "    testFeaturesForPassword = np.array(testFeaturesForPassword)\n",
    "    testLabels = np.array(testLabels)\n",
    "\n",
    "def _getTopProbabilities(classifier, predictionProbabilityArray):\n",
    "    return dict(sorted(dict(zip(classifier.classes_,predictionProbabilityArray)).items(), key=lambda x:x[1], reverse=True))\n",
    "    \n",
    "def get_top_digraphs(classifier,predictionProbabilityArray, numberOfDigraphsToPredict=10):\n",
    "    return list(_getTopProbabilities(classifier,predictionProbabilityArray).keys())[:numberOfDigraphsToPredict]\n",
    "\n",
    "def calculatePenaltyScore(digraphProbabilites, testLabels):\n",
    "    \"\"\" Calculate the penalty score for a password given a list of digraph probabilities \"\"\"\n",
    "    penaltyScore = 0\n",
    "    diCount = 0\n",
    "    for i in range(len(digraphProbabilites)): \n",
    "        row = digraphProbabilites[i]\n",
    "        for j in range(len(row)):\n",
    "            if row[j]==testLabels[i]:\n",
    "                diCount+=1\n",
    "                penaltyScore += (j+1)\n",
    "                break\n",
    "    if diCount!=7:\n",
    "        print(\"Error. Digraph count = {diCount}\")\n",
    "    return penaltyScore \n",
    "\n",
    "\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "def _getXTestWithThreshold(xTestCopyForThreshold, thresholdValue):\n",
    "    xTestWithOffset = []\n",
    "    for row in xTestCopyForThreshold:\n",
    "        newRow = row\n",
    "        if abs(row[0]) < thresholdValue:\n",
    "              newRow = [thresholdValue]\n",
    "        xTestWithOffset.append(newRow)\n",
    "          \n",
    "    xTestWithOffset = np.array(xTestWithOffset)\n",
    "    return xTestWithOffset \n",
    "\n",
    "\n",
    "# Executes one test run\n",
    "def _getPenaltyScore(password, digraphProbabilities):\n",
    "    curPasswordDigraph = []\n",
    "    for i in range(1,len(password)):\n",
    "        curPasswordDigraph.append(password[i-1:i+1])   \n",
    "    return calculatePenaltyScore(digraphProbabilities, curPasswordDigraph)\n",
    "\n",
    "def _getDigraphProbabilties(xTest, yTest, testPassword):\n",
    "    # Get random features \n",
    "    testFeatures, testLabels = _getFeaturesAndLabelsForPassword(testPassword, xTest, yTest)  \n",
    "    predictedProbabilites = classifier.predict_proba(testFeatures)\n",
    "    # Predict \n",
    "    digraphProbabilities=[]\n",
    "    for row in predictedProbabilites:\n",
    "        digraphProbabilities.append(get_top_digraphs(classifier,row, 556))\n",
    "    \n",
    "    return digraphProbabilities\n",
    "    \n",
    "\n",
    "def _testRun(poolData):\n",
    "    # Data \n",
    "    xTest, yTest, testPassword, isThreshold = poolData\n",
    "    label = \"WithThreshold\" if isThreshold else \"Without\"\n",
    "    randomGuess = relevantRockYouPasswords.index(testPassword)\n",
    "    res = [testPassword, label, randomGuess]\n",
    "    bestGuesses = []\n",
    "\n",
    "    # Get the required features and probabilites\n",
    "    testFeatures, testLabels = _getFeaturesAndLabelsForPassword(testPassword, xTest, yTest)  \n",
    "    predictedProbabilites = classifier.predict_proba(testFeatures)\n",
    "    \n",
    "    digraphProbabilities=[]\n",
    "    for row in predictedProbabilites:\n",
    "        digraphProbabilities.append(get_top_digraphs(classifier,row, 556))\n",
    "\n",
    "    # Generate penalty scores for each \n",
    "    penaltyScores = {}\n",
    "    for index, password in enumerate(relevantRockYouPasswords):\n",
    "        penaltyScores[password] = _getPenaltyScore(password, digraphProbabilities)\n",
    "\n",
    "    return penaltyScores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "\n",
    "xTestWithThreshold = {}\n",
    "thresholdValues = range(100, 350, 25)\n",
    "\n",
    "for threshold in thresholdValues: \n",
    "    xTestWithThreshold[threshold] = dask.delayed(_getXTestWithThreshold)(xTestCopyForThreshold, 100) \n",
    "\n",
    "xTestWithThreshold = dask.compute(xTestWithThreshold)\n",
    "xTestWithThreshold = xTestWithThreshold[0]\n",
    "\n",
    "# Generate test passwords \n",
    "n = 3\n",
    "testPasswords = random.sample(relevantRockYouPasswords, n)\n",
    "\n",
    "from itertools import product \n",
    "\n",
    "params = list(product(testPasswords, thresholdValues))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.56 ms, sys: 3.23 ms, total: 12.8 ms\n",
      "Wall time: 13.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "futures = []\n",
    "for param in params: \n",
    "    password, threshold = param\n",
    "    poolData = [xTestWithThreshold[threshold], yTest, password, True]\n",
    "    future = dask.delayed(_testRun)(poolData)\n",
    "    futures.append(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if __name__ == '__main__':\n",
    "    res = dask.compute(futures) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'futures' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d661b3daa682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfutures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'futures' is not defined"
     ]
    }
   ],
   "source": [
    "futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
